{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkTDEJuIs3Tl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Flatten, Activation, BatchNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#Note we have tried different parameters for different combinations however for just one code we have provided this condensed code\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "epochs = 25\n",
        "iterations = 782\n",
        "DATA_FORMAT = 'channels_last'\n",
        "\n",
        "# Define the directory for TensorBoard logs\n",
        "log_dir = './logs'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "log_filepath = os.path.join(log_dir, 'tensorboard_logs')\n",
        "\n",
        "def color_preprocessing(x_train, x_test):\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    x_test = x_test.astype('float32') / 255\n",
        "    return x_train, x_test\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 100:\n",
        "        return 0.01\n",
        "    # if epoch < 200:\n",
        "    #     return 0.001\n",
        "    return 0.0001\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train, x_test = color_preprocessing(x_train, x_test)\n",
        "\n",
        "def medium_deep_net(img_input, activation, dropout_rate=0.5, optimizer=None):\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation=activation)(img_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(dropout_rate)(x)  # Dropout after Conv2D\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(dropout_rate)(x)  # Dropout after Conv2D\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(256, activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)  # Dropout after Dense\n",
        "    x = Dense(128, activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)  # Dropout after Dense\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    if optimizer:\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_model(model, epochs=epochs):\n",
        "    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
        "    change_lr = LearningRateScheduler(scheduler)\n",
        "    cbks = [change_lr, tb_cb]\n",
        "    model.summary()\n",
        "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=cbks, validation_data=(x_test, y_test))\n",
        "    return history\n",
        "\n",
        "activation_functions = ['relu', 'tanh', 'sigmoid', 'gelu']\n",
        "optimizers_list = [\n",
        "    {'name': 'SGD with Momentum', 'optimizer': optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)},\n",
        "    {'name': 'SGD without Momentum', 'optimizer': optimizers.SGD(lr=0.01)},\n",
        "    {'name': 'RMSprop', 'optimizer': optimizers.RMSprop()},\n",
        "    {'name': 'Adam', 'optimizer': optimizers.Adam()},\n",
        "]\n",
        "dropout_rates = [0.25, 0.5, 0.75]  # Define different dropout rates to try\n",
        "\n",
        "for activation in activation_functions:\n",
        "    for dropout_rate in dropout_rates:\n",
        "        for opt in optimizers_list:\n",
        "            print(f\"Training model with activation: {activation}, dropout rate: {dropout_rate}, optimizer: {opt['name']}\")\n",
        "            model = medium_deep_net(Input(shape=(32, 32, 3)), activation=activation, dropout_rate=dropout_rate, optimizer=opt['optimizer'])\n",
        "            history = train_model(model)\n",
        "\n",
        "            # Evaluate the model on test data\n",
        "            test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "            print(f'Test accuracy with {activation} activation, dropout rate {dropout_rate}, and {opt[\"name\"]} optimizer: {test_acc}')\n",
        "\n",
        "print('Experimentation completed!')\n"
      ]
    }
  ]
}