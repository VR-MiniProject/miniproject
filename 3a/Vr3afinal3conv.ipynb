{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Input, Conv2D, Dense, MaxPooling2D, Flatten, Activation, BatchNormalization, Dropout\n",
        "from keras.models import Model\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#In this code we have set the parameters to our best performing model\n",
        "\n",
        "\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "epochs = 25\n",
        "iterations = 782\n",
        "DROPOUT = 0.5\n",
        "DATA_FORMAT = 'channels_last'\n",
        "\n",
        "\n",
        "\n",
        "def color_preprocessing(x_train, x_test):\n",
        "    x_train = x_train.astype('float32') / 255\n",
        "    x_test = x_test.astype('float32') / 255\n",
        "    return x_train, x_test\n",
        "\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 100:\n",
        "        return 0.01\n",
        "    # if epoch < 200:\n",
        "    #     return 0.001\n",
        "    return 0.0001\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train, x_test = color_preprocessing(x_train, x_test)\n",
        "\n",
        "\n",
        "def medium_deep_net_with_extra_conv(img_input, activation, optimizer=None):\n",
        "    x = Conv2D(32, (3, 3), padding='same', activation=activation)(img_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(0.25)(x)  # Dropout after Conv2D\n",
        "\n",
        "    x = Conv2D(64, (3, 3), padding='same', activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(0.25)(x)  # Dropout after Conv2D\n",
        "\n",
        "    x = Conv2D(128, (3, 3), padding='same', activation=activation)(x)  # New Conv2D layer\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Dropout(0.25)(x)  # Dropout after Conv2D\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(256, activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)  # Dropout after Dense\n",
        "    x = Dense(128, activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)  # Dropout after Dense\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    if optimizer:\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(model, epochs=epochs):\n",
        "    tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
        "    change_lr = LearningRateScheduler(scheduler)\n",
        "    cbks = [change_lr, tb_cb]\n",
        "    model.summary()\n",
        "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, callbacks=cbks, validation_data=(x_test, y_test))\n",
        "    return history\n",
        "\n",
        "\n",
        "activation_functions = ['gelu']\n",
        "optimizers_list = [\n",
        "   #   {'name': 'SGD with Momentum', 'optimizer': optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)},\n",
        "   # {'name': 'SGD without Momentum', 'optimizer': optimizers.SGD(lr=0.01)},\n",
        "   # {'name': 'RMSprop', 'optimizer': optimizers.RMSprop()},\n",
        "    {'name': 'Adam', 'optimizer': optimizers.Adam()},\n",
        "]\n",
        "\n",
        "for activation in activation_functions:\n",
        "    for opt in optimizers_list:\n",
        "        print(f\"Training model with activation: {activation}, optimizer: {opt['name']}\")\n",
        "        model = medium_deep_net_with_extra_conv(Input(shape=(32, 32, 3)), activation=activation, optimizer=opt['optimizer'])\n",
        "        history = train_model(model)\n",
        "\n",
        "        # Evaluate the model on test data\n",
        "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print(f'Test accuracy with {activation} activation and {opt[\"name\"]} optimizer: {test_acc}')\n",
        "\n",
        "print('Experimentation completed!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d19wDCcH3XAM",
        "outputId": "c9b71b3f-ef74-40be-c939-010266775b4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with activation: gelu, optimizer: Adam\n",
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_26 (Conv2D)          (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_46 (Ba  (None, 32, 32, 32)        128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_26 (MaxPooli  (None, 16, 16, 32)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_47 (Ba  (None, 16, 16, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPooli  (None, 8, 8, 64)          0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_48 (Ba  (None, 8, 8, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPooli  (None, 4, 4, 128)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 256)               524544    \n",
            "                                                                 \n",
            " batch_normalization_49 (Ba  (None, 256)               1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_50 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_50 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 654410 (2.50 MB)\n",
            "Trainable params: 653194 (2.49 MB)\n",
            "Non-trainable params: 1216 (4.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "782/782 [==============================] - 187s 235ms/step - loss: 1.5846 - accuracy: 0.4312 - val_loss: 1.9890 - val_accuracy: 0.3705 - lr: 0.0100\n",
            "Epoch 2/25\n",
            "782/782 [==============================] - 181s 232ms/step - loss: 1.2001 - accuracy: 0.5770 - val_loss: 1.9319 - val_accuracy: 0.4130 - lr: 0.0100\n",
            "Epoch 3/25\n",
            "782/782 [==============================] - 181s 232ms/step - loss: 1.0641 - accuracy: 0.6305 - val_loss: 1.2786 - val_accuracy: 0.5646 - lr: 0.0100\n",
            "Epoch 4/25\n",
            "782/782 [==============================] - 187s 240ms/step - loss: 0.9803 - accuracy: 0.6629 - val_loss: 1.0981 - val_accuracy: 0.6308 - lr: 0.0100\n",
            "Epoch 5/25\n",
            "782/782 [==============================] - 185s 236ms/step - loss: 0.9225 - accuracy: 0.6835 - val_loss: 0.8232 - val_accuracy: 0.7172 - lr: 0.0100\n",
            "Epoch 6/25\n",
            "782/782 [==============================] - 185s 236ms/step - loss: 0.8865 - accuracy: 0.6977 - val_loss: 0.9165 - val_accuracy: 0.6824 - lr: 0.0100\n",
            "Epoch 7/25\n",
            "782/782 [==============================] - 185s 236ms/step - loss: 0.8436 - accuracy: 0.7109 - val_loss: 0.8226 - val_accuracy: 0.7153 - lr: 0.0100\n",
            "Epoch 8/25\n",
            "782/782 [==============================] - 182s 233ms/step - loss: 0.8160 - accuracy: 0.7209 - val_loss: 0.8108 - val_accuracy: 0.7169 - lr: 0.0100\n",
            "Epoch 9/25\n",
            "782/782 [==============================] - 179s 229ms/step - loss: 0.7887 - accuracy: 0.7321 - val_loss: 0.8521 - val_accuracy: 0.7085 - lr: 0.0100\n",
            "Epoch 10/25\n",
            "782/782 [==============================] - 181s 231ms/step - loss: 0.7638 - accuracy: 0.7421 - val_loss: 0.9695 - val_accuracy: 0.6750 - lr: 0.0100\n",
            "Epoch 11/25\n",
            "782/782 [==============================] - 182s 232ms/step - loss: 0.7445 - accuracy: 0.7472 - val_loss: 0.6733 - val_accuracy: 0.7666 - lr: 0.0100\n",
            "Epoch 12/25\n",
            "782/782 [==============================] - 182s 232ms/step - loss: 0.7245 - accuracy: 0.7541 - val_loss: 0.8012 - val_accuracy: 0.7220 - lr: 0.0100\n",
            "Epoch 13/25\n",
            "782/782 [==============================] - 185s 236ms/step - loss: 0.7068 - accuracy: 0.7609 - val_loss: 0.8215 - val_accuracy: 0.7188 - lr: 0.0100\n",
            "Epoch 14/25\n",
            "782/782 [==============================] - 185s 237ms/step - loss: 0.6912 - accuracy: 0.7671 - val_loss: 0.7128 - val_accuracy: 0.7622 - lr: 0.0100\n",
            "Epoch 15/25\n",
            "782/782 [==============================] - 184s 236ms/step - loss: 0.6715 - accuracy: 0.7727 - val_loss: 0.7293 - val_accuracy: 0.7496 - lr: 0.0100\n",
            "Epoch 16/25\n",
            "782/782 [==============================] - 185s 237ms/step - loss: 0.6642 - accuracy: 0.7744 - val_loss: 0.8697 - val_accuracy: 0.7209 - lr: 0.0100\n",
            "Epoch 17/25\n",
            "782/782 [==============================] - 180s 230ms/step - loss: 0.6512 - accuracy: 0.7813 - val_loss: 0.6819 - val_accuracy: 0.7664 - lr: 0.0100\n",
            "Epoch 18/25\n",
            "782/782 [==============================] - 183s 234ms/step - loss: 0.6384 - accuracy: 0.7835 - val_loss: 0.6276 - val_accuracy: 0.7890 - lr: 0.0100\n",
            "Epoch 19/25\n",
            "782/782 [==============================] - 184s 235ms/step - loss: 0.6255 - accuracy: 0.7887 - val_loss: 0.7426 - val_accuracy: 0.7495 - lr: 0.0100\n",
            "Epoch 20/25\n",
            "782/782 [==============================] - 183s 233ms/step - loss: 0.6144 - accuracy: 0.7953 - val_loss: 0.6322 - val_accuracy: 0.7883 - lr: 0.0100\n",
            "Epoch 21/25\n",
            "782/782 [==============================] - 183s 234ms/step - loss: 0.6093 - accuracy: 0.7941 - val_loss: 0.6537 - val_accuracy: 0.7769 - lr: 0.0100\n",
            "Epoch 22/25\n",
            "782/782 [==============================] - 182s 233ms/step - loss: 0.5999 - accuracy: 0.7957 - val_loss: 0.7728 - val_accuracy: 0.7466 - lr: 0.0100\n",
            "Epoch 23/25\n",
            "782/782 [==============================] - 182s 233ms/step - loss: 0.5874 - accuracy: 0.8020 - val_loss: 0.7138 - val_accuracy: 0.7617 - lr: 0.0100\n",
            "Epoch 24/25\n",
            "782/782 [==============================] - 184s 235ms/step - loss: 0.5793 - accuracy: 0.8053 - val_loss: 0.7328 - val_accuracy: 0.7615 - lr: 0.0100\n",
            "Epoch 25/25\n",
            "782/782 [==============================] - 183s 235ms/step - loss: 0.5749 - accuracy: 0.8072 - val_loss: 0.6302 - val_accuracy: 0.7949 - lr: 0.0100\n",
            "Test accuracy with gelu activation and Adam optimizer: 0.7949000000953674\n",
            "Experimentation completed!\n"
          ]
        }
      ]
    }
  ]
}